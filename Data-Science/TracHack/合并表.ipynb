{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==1.0.3 in /opt/conda/lib/python3.7/site-packages (1.0.3)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas==1.0.3) (2020.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from pandas==1.0.3) (1.19.2)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas==1.0.3) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas==1.0.3) (1.14.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# installing 1.0.3 because this version of pandas supports write to s3\n",
    "!pip install pandas==1.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This path will be active after the launch of the hackathon / day-day-up-unsw / s3://tf-trachack-notebooks/day-day-up-unsw/jupyter/jovyan/\n",
    "teamname = 'day-day-up-unsw' #'trachack-a-groups-admin-py-tracfone'\n",
    "data_folder='s3://tf-trachack-data/212/'\n",
    "# change root_folder to your team's root folder\n",
    "# s3://tf-trachack-notebooks/<this should be replaced by team name as provided in EMAIL>/jupyter/jovyan/\n",
    "root_folder='s3://tf-trachack-notebooks/'+teamname+'/jupyter/jovyan/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from datetime import datetime\n",
    "from pandas import to_datetime\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import KNNImputer,SimpleImputer\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upgrades=pd.read_csv(data_folder+\"data/dev/upgrades.csv\")\n",
    "# upgrades_eval=pd.read_csv(data_folder+\"data/eval/upgrades.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_info = pd.read_csv(root_folder+\"code/data/dev/phone_info.csv\")\n",
    "phone_info_eval = pd.read_csv(root_folder+\"code/data/eval/phone_info_eval.csv\")\n",
    "train_set = pd.read_csv(root_folder+\"code/data/dev/upgrade_redemptions_deactivations_reactivations_suspensions.csv\")\n",
    "eval_set = pd.read_csv(root_folder+\"code/data/eval/upgrade_redemptions_deactivations_reactivations_suspensions_eval.csv\")\n",
    "customer_info = pd.read_csv(root_folder+\"code/data/dev/customer_info_new.csv\")\n",
    "customer_info_eval = pd.read_csv(root_folder+\"code/data/eval/customer_info_new_eval.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set = pd.merge(upgrades, train_set, on='line_id')\n",
    "# eval_set = pd.merge(eval_set, phone_info, on='line_id')\n",
    "train_set = pd.merge(train_set, phone_info, on='line_id')\n",
    "train_set = pd.merge(train_set, customer_info, on='line_id')\n",
    "eval_set = pd.merge(eval_set, phone_info_eval, on='line_id')\n",
    "eval_set = pd.merge(eval_set, customer_info_eval, on='line_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 55868 entries, 0 to 55867\n",
      "Data columns (total 46 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   line_id                    55868 non-null  object \n",
      " 1   upgrade                    55868 non-null  float64\n",
      " 2   redemption_year            55868 non-null  float64\n",
      " 3   redemption_month           55868 non-null  float64\n",
      " 4   redemption_day             55868 non-null  float64\n",
      " 5   redemptions_0              55868 non-null  float64\n",
      " 6   redemptions_1              55868 non-null  float64\n",
      " 7   redemptions_2              55868 non-null  float64\n",
      " 8   redemptions_3              55868 non-null  float64\n",
      " 9   redemptions_4              55868 non-null  float64\n",
      " 10  redemptions_5              55868 non-null  float64\n",
      " 11  ave_gross_revenue          55868 non-null  float64\n",
      " 12  deactivation_year          55868 non-null  float64\n",
      " 13  deactivation_month         55868 non-null  float64\n",
      " 14  deactivation_day           55868 non-null  float64\n",
      " 15  reactivation_year          55868 non-null  float64\n",
      " 16  reactivation_month         55868 non-null  float64\n",
      " 17  reactivation_day           55868 non-null  float64\n",
      " 18  suspension_start_year      55868 non-null  float64\n",
      " 19  suspension_start_month     55868 non-null  float64\n",
      " 20  suspension_start_day       55868 non-null  float64\n",
      " 21  suspension_end_year        55868 non-null  float64\n",
      " 22  suspension_end_month       55868 non-null  float64\n",
      " 23  suspension_end_day         55868 non-null  float64\n",
      " 24  suspension_year            55868 non-null  float64\n",
      " 25  suspension_month           55868 non-null  float64\n",
      " 26  suspension_day             55868 non-null  float64\n",
      " 27  cpu_cores                  55868 non-null  float64\n",
      " 28  lte_category               55868 non-null  float64\n",
      " 29  year_released              55868 non-null  float64\n",
      " 30  year_released_1            55868 non-null  float64\n",
      " 31  first_activation_year      55868 non-null  float64\n",
      " 32  first_activation_month     55868 non-null  float64\n",
      " 33  first_activation_day       55868 non-null  float64\n",
      " 34  customer_redemption_year   55868 non-null  float64\n",
      " 35  customer_redemption_month  55868 non-null  float64\n",
      " 36  customer_redemption_day    55868 non-null  float64\n",
      " 37  plan_0                     55868 non-null  float64\n",
      " 38  plan_1                     55868 non-null  float64\n",
      " 39  plan_2                     55868 non-null  float64\n",
      " 40  plan_3                     55868 non-null  float64\n",
      " 41  plan_4                     55868 non-null  float64\n",
      " 42  plan_5                     55868 non-null  float64\n",
      " 43  carrier_0                  55868 non-null  float64\n",
      " 44  carrier_1                  55868 non-null  float64\n",
      " 45  carrier_2                  55868 non-null  float64\n",
      "dtypes: float64(45), object(1)\n",
      "memory usage: 20.0+ MB\n"
     ]
    }
   ],
   "source": [
    "train_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line_id</th>\n",
       "      <th>redemption_year</th>\n",
       "      <th>redemption_month</th>\n",
       "      <th>redemption_day</th>\n",
       "      <th>redemptions_0</th>\n",
       "      <th>redemptions_1</th>\n",
       "      <th>redemptions_2</th>\n",
       "      <th>redemptions_3</th>\n",
       "      <th>redemptions_4</th>\n",
       "      <th>redemptions_5</th>\n",
       "      <th>...</th>\n",
       "      <th>customer_redemption_day</th>\n",
       "      <th>plan_0</th>\n",
       "      <th>plan_1</th>\n",
       "      <th>plan_2</th>\n",
       "      <th>plan_3</th>\n",
       "      <th>plan_4</th>\n",
       "      <th>plan_5</th>\n",
       "      <th>carrier_0</th>\n",
       "      <th>carrier_1</th>\n",
       "      <th>carrier_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>584f0d75-5897-4e56-b475-0b3adf1aeb07</td>\n",
       "      <td>1.421779</td>\n",
       "      <td>1.473173</td>\n",
       "      <td>1.560174</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.784983</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e879756a-daff-4f72-9206-d96009861ac1</td>\n",
       "      <td>1.421779</td>\n",
       "      <td>1.473173</td>\n",
       "      <td>1.560174</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.671055</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cbb0795d-716a-44f1-bc82-f8f8cb5db002</td>\n",
       "      <td>1.421779</td>\n",
       "      <td>1.204974</td>\n",
       "      <td>-0.752175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.037875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e1e914fd-1e04-4148-9251-ed6a4dd6d3e7</td>\n",
       "      <td>1.421779</td>\n",
       "      <td>1.473173</td>\n",
       "      <td>1.560174</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.126769</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1987025d-39be-4120-a09a-8d89c0e8d151</td>\n",
       "      <td>1.421779</td>\n",
       "      <td>1.473173</td>\n",
       "      <td>-1.067496</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.379660</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                line_id  redemption_year  redemption_month  \\\n",
       "0  584f0d75-5897-4e56-b475-0b3adf1aeb07         1.421779          1.473173   \n",
       "1  e879756a-daff-4f72-9206-d96009861ac1         1.421779          1.473173   \n",
       "2  cbb0795d-716a-44f1-bc82-f8f8cb5db002         1.421779          1.204974   \n",
       "3  e1e914fd-1e04-4148-9251-ed6a4dd6d3e7         1.421779          1.473173   \n",
       "4  1987025d-39be-4120-a09a-8d89c0e8d151         1.421779          1.473173   \n",
       "\n",
       "   redemption_day  redemptions_0  redemptions_1  redemptions_2  redemptions_3  \\\n",
       "0        1.560174            0.0            0.0            0.0            1.0   \n",
       "1        1.560174            0.0            0.0            0.0            1.0   \n",
       "2       -0.752175            0.0            0.0            0.0            1.0   \n",
       "3        1.560174            0.0            1.0            0.0            1.0   \n",
       "4       -1.067496            0.0            0.0            0.0            1.0   \n",
       "\n",
       "   redemptions_4  redemptions_5  ...  customer_redemption_day  plan_0  plan_1  \\\n",
       "0            0.0            0.0  ...                 0.784983     0.0     0.0   \n",
       "1            0.0            0.0  ...                 0.671055     0.0     0.0   \n",
       "2            0.0            0.0  ...                -1.037875     0.0     0.0   \n",
       "3            0.0            0.0  ...                 1.126769     0.0     1.0   \n",
       "4            0.0            0.0  ...                -1.379660     0.0     0.0   \n",
       "\n",
       "   plan_2  plan_3  plan_4  plan_5  carrier_0  carrier_1  carrier_2  \n",
       "0     0.0     1.0     0.0     0.0        1.0        0.0        0.0  \n",
       "1     0.0     1.0     0.0     0.0        1.0        0.0        0.0  \n",
       "2     1.0     0.0     0.0     0.0        1.0        0.0        0.0  \n",
       "3     0.0     0.0     0.0     0.0        1.0        0.0        0.0  \n",
       "4     1.0     0.0     0.0     0.0        1.0        0.0        0.0  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_set.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.to_csv(root_folder+\"Hanxi Liu/train_set.csv\", header=True, index=None)\n",
    "eval_set.to_csv(root_folder+\"Hanxi Liu/eval_set.csv\", header=True, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score, recall_score, accuracy_score, precision_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_data(data_frame):\n",
    "    '''Selects certain features from raw GCR records.'''\n",
    "    \n",
    "#     df_loc = fields.append('upgrade')\n",
    "    df_selection = data_frame.iloc[:, 1:]\n",
    "    df_selection.drop(columns=['redemption_day', 'customer_redemption_day', 'suspension_end_day', 'suspension_day', 'suspension_month'])\n",
    "#     df_selection = data_frame.loc[:,[\"available_online_N\",\"available_online_Y\",\"battery_removable_N\",\"battery_removable_Y\",\"data_capable_1.0\",\"data_capable_N\",\"unlock_elegible_N\",\"unlock_elegible_Y\",\"bluetooth_N\",\"bluetooth_Y\",\"extd_warranty_1.0\",\"extd_warranty_N\",\"upgrade\"]]\n",
    "    return df_selection\n",
    "\n",
    "def select_eval_data(data_frame):\n",
    "    '''Selects certain features from raw GCR records.'''\n",
    "    df_selection = data_frame\n",
    "    df_selection.drop(columns=['redemption_day', 'customer_redemption_day', 'suspension_end_day','suspension_day','suspension_month'])\n",
    "#     df_selection.drop(columns=['redemption_day', 'customer_redemption_day', 'suspension_end_day'])\n",
    "#     df_selection = df_selection.drop(columns=['date_observed', 'suspension_start_date', 'suspension_end_date'])\n",
    "#     df_selection = data_frame.loc[:,[\"line_id\", \"available_online_N\",\"available_online_Y\",\"battery_removable_N\",\"battery_removable_Y\",\"data_capable_1.0\",\"data_capable_N\",\"unlock_elegible_N\",\"unlock_elegible_Y\",\"bluetooth_N\",\"bluetooth_Y\",\"extd_warranty_1.0\",\"extd_warranty_N\"]]\n",
    "    return df_selection\n",
    "\n",
    "def get_usage_summary(df):\n",
    "    '''Returns aggregate data usage for a line_id.'''\n",
    "    data_kb= df.loc[:,['line_id', \"total_kb\"]]\n",
    "    data_kb = data_kb.groupby('line_id')['total_kb'].sum().reset_index()\n",
    "    data_kb.columns=['line_id','sum_total_kb']\n",
    "    return data_kb\n",
    "\n",
    "def preprocess_customer_info(df):\n",
    "    '''Imputes null values in carrier and plan_name column with most frequent value.'''\n",
    "    customer_info=df.loc[:,['line_id','carrier','plan_name']].drop_duplicates()\n",
    "    customer_info['carrier'].fillna(customer_info['carrier'].mode()[0], inplace=True)\n",
    "    customer_info['plan_name'].fillna(customer_info['plan_name'].mode()[0], inplace=True)\n",
    "    customer_info=pd.get_dummies(customer_info,columns=['carrier','plan_name'],drop_first=True)\n",
    "    return customer_info\n",
    "\n",
    "def train_model(train_X,train_Y):\n",
    "    '''Given a preprocessed training dataset, trains a simple logistic regression model and \n",
    "    returns the trained model object'''\n",
    "#     lr_recipe = LogisticRegression(fit_intercept=True,class_weight='balanced')\n",
    "#     lr_model = lr_recipe.fit(train_X,train_Y)\n",
    "#     lr_recipe = RandomForestClassifier(max_depth=8, min_samples_leaf=20, min_samples_split=10,\n",
    "#                         n_estimators=11, random_state=19)\n",
    "#     lr_recipe = RandomForestClassifier(random_state=0,class_weight='balanced')\n",
    "    rfc = RandomForestClassifier(max_depth=8, min_samples_leaf=20, min_samples_split=10,\n",
    "                            n_estimators=11, random_state=19)\n",
    "#     for n in range(2,10):\n",
    "#         scores = cross_val_score(rfc, train_X, train_Y, cv=n)\n",
    "#         print(n, scores, np.mean(scores))\n",
    "        \n",
    "#     ada = AdaBoostClassifier(random_state=19)\n",
    "    lr_model = rfc.fit(train_X,train_Y)\n",
    "#     lr_model = ada.fit(train_X,train_Y)\n",
    "    return lr_model\n",
    "\n",
    "def evaluate_model(model, test_X,test_Y):\n",
    "    '''Given a model and preprocessed test dataset, returns the f1 value'''\n",
    "    y_pred=model.predict(test_X)\n",
    "#     f1 = f1_score(test_Y, y_pred, average='macro')\n",
    "    f1 = f1_score(test_Y, y_pred)\n",
    "    print('recall: ', recall_score(test_Y, y_pred))\n",
    "    print('accuracy: ', accuracy_score(test_Y, y_pred))\n",
    "    print('precision: ', precision_score(test_Y, y_pred))\n",
    "    return round(f1,3)\n",
    "\n",
    "def main(data_path, random_seed, test_ratio=0.3):\n",
    "    '''The end to end model pipeline'''\n",
    "    df =pd.read_csv(data_path,low_memory=True)    \n",
    "    df_selected = select_data(df)\n",
    "#     usage_summary=get_usage_summary(df_selected)\n",
    "#     customer_info=preprocess_customer_info(df_selected)\n",
    "#     line_ids=df_selected.loc[:,['line_id','upgrade']].drop_duplicates().reset_index(drop=True)\n",
    "#     df_preprocessed=pd.merge(line_ids,usage_summary,on='line_id',how='inner')\n",
    "#     df_preprocessed=pd.merge(df_preprocessed,customer_info,on='line_id')\n",
    "#     df_Y=df_preprocessed['upgrade'].replace({'yes':1,'no':0})\n",
    "#     df_X=df_preprocessed.drop(columns=['line_id','upgrade'])\n",
    "#     print(df_selected.head(5))\n",
    "    df_Y = df_selected.loc[:, ['upgrade']]\n",
    "    df_X = df_selected.drop(columns=['upgrade'])\n",
    "#     print(df_Y.head(5))\n",
    "#     df_X  = df_selected.iloc[:, 0:-1]\n",
    "#     df_Y = df_selected.iloc[:, -1]\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(df_X, df_Y, test_size=test_ratio, random_state=random_seed)\n",
    "    \n",
    "    num_train = len(X_train)\n",
    "    num_test = len(X_test)\n",
    "    print(f\"Train has {num_train}\")\n",
    "    print(f\"Test has {num_test}\")\n",
    "    model = train_model(X_train,Y_train)\n",
    "    f1 = evaluate_model(model, X_test, Y_test)\n",
    "    return model,f1\n",
    "\n",
    "def make_predictions(model,eval_data_path,submission_path):\n",
    "    '''Given a model, eval data path and submission path, makes predictions and \n",
    "    saves the submissions to submission path.'''\n",
    "#     eval_data = pd.read_csv(eval_data_path,low_memory=True) \n",
    "#     eval_data = pd.read_csv(eval_data_path) \n",
    "    eval_data = eval_set\n",
    "    df_selected = select_eval_data(eval_data)\n",
    "#     usage_summary=get_usage_summary(df_selected)\n",
    "#     customer_info=preprocess_customer_info(df_selected)\n",
    "    line_ids=df_selected.loc[:,['line_id']].drop_duplicates().reset_index(drop=True)\n",
    "#     df_preprocessed=pd.merge(line_ids,usage_summary,on='line_id',how='inner')\n",
    "#     df_preprocessed=pd.merge(df_preprocessed,customer_info,on='line_id')\n",
    "#     eval_X=df_preprocessed.drop(columns=['line_id'])\n",
    "    eval_X=df_selected.drop(columns=['line_id'])\n",
    "    predictions=pd.DataFrame(line_ids,columns=['line_id'])\n",
    "    predictions['upgrade']=model.predict(eval_X)\n",
    "    #predictions.to_csv(submission_path,header=True,index=None)\n",
    "    #print(f\"submission saved to {submission_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train has 39107\n",
      "Test has 16761\n",
      "recall:  0.7865612648221344\n",
      "accuracy:  0.8871785692977746\n",
      "precision:  0.7958231504110198\n",
      "f1-score: 0.791\n"
     ]
    }
   ],
   "source": [
    "# This path will be active after the launch of the hackathon  / day-day-up-unsw / s3://tf-trachack-notebooks/day-day-up-unsw/jupyter/jovyan/\n",
    "teamname = 'day-day-up-unsw' # 'trachack-a-groups-admin-py-tracfone'\n",
    "data_folder= 's3://tf-trachack-data/212/'\n",
    "# change root_folder to your team's root folder\n",
    "# s3://tf-trachack-notebooks/<this should be replaced by team name as provided in EMAIL>/jupyter/jovyan/\n",
    "root_folder='s3://tf-trachack-notebooks/'+teamname+'/jupyter/jovyan/'\n",
    "#data_path = root_folder+'Hanxi Liu/dev-sample.csv'\n",
    "data_path = root_folder+\"Hanxi Liu/train_set.csv\"\n",
    "seed = 125\n",
    "model,f1 = main(data_path, seed)\n",
    "print(f\"f1-score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
